{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shangeth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in range(0,df.shape[0]):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine learning models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shangeth/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=1500, units=6, kernel_initializer=\"uniform\")`\n",
      "/home/shangeth/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "/home/shangeth/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "/home/shangeth/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "800/800 [==============================] - 0s 496us/step - loss: 0.6931 - acc: 0.5088\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.6898 - acc: 0.7438\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.6550 - acc: 0.8362\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.5470 - acc: 0.8750\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.4056 - acc: 0.9150\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.2937 - acc: 0.9312\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.2166 - acc: 0.9487\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.1671 - acc: 0.9637\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.1326 - acc: 0.9737\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 114us/step - loss: 0.1084 - acc: 0.9737\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.0900 - acc: 0.9787\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.0767 - acc: 0.9812\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.0671 - acc: 0.9850\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.0595 - acc: 0.9862\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.0534 - acc: 0.9887\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.0490 - acc: 0.9887\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.0449 - acc: 0.9912\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.0412 - acc: 0.9912\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.0389 - acc: 0.9912\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.0359 - acc: 0.9900\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.0343 - acc: 0.9912\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 114us/step - loss: 0.0325 - acc: 0.9912\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.0307 - acc: 0.9925\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.0293 - acc: 0.9925\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.0279 - acc: 0.9925\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.0274 - acc: 0.9912\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.0261 - acc: 0.9912\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.0258 - acc: 0.9912\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.0244 - acc: 0.9912\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.0239 - acc: 0.9925\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.0233 - acc: 0.9925\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.0227 - acc: 0.9900\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.0224 - acc: 0.9912\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.0215 - acc: 0.9900\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.0212 - acc: 0.9912\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.0205 - acc: 0.9900\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.0210 - acc: 0.9900\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.0201 - acc: 0.9900\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.0195 - acc: 0.9912\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 114us/step - loss: 0.0198 - acc: 0.9912\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.0198 - acc: 0.9900\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.0185 - acc: 0.9912\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.0187 - acc: 0.9925\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.0185 - acc: 0.9912\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.0180 - acc: 0.9912\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.0188 - acc: 0.9925\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.0180 - acc: 0.9912\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.0177 - acc: 0.9900\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.0173 - acc: 0.9925\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.0177 - acc: 0.9925\n",
      "Accuracy =  0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\"\"\"Use any classification model for training\"\"\"\n",
    "# reg = DecisionTreeRegressor(random_state=0)\n",
    "# reg = GaussianNB()\n",
    "\n",
    "# reg = MLPClassifier(hidden_layer_sizes=(1000,1000,1000),max_iter=20000,random_state=0)\n",
    "# reg.fit(X_train,y_train)\n",
    "# y_pred = reg.predict(X_test)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "clf = Sequential()\n",
    "\n",
    "#1st hidden layer\n",
    "clf.add(Dense(output_dim=6, init='uniform', activation='relu', input_dim=1500))\n",
    "\n",
    "#2nd hidden layer\n",
    "clf.add(Dense(output_dim=6, init='uniform', activation='relu'))\n",
    "\n",
    "#output layer\n",
    "clf.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
    "\n",
    "#applying GD to the NN\n",
    "    #classifier = adam stochastic gradient descent\n",
    "    #loss_function = binary_cross_entrophy(logarithmic loss function)\n",
    "    # metric = accuracy\n",
    "clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "clf.fit(X_train, y_train, batch_size=10, nb_epoch=50)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = (y_pred>0.5) \n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy = ',accuracy_score(y_pred,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix = \n",
      " [[69 28]\n",
      " [22 81]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix = \\n',cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reaction(comment):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', comment)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    cor1 = corpus\n",
    "    cor1.append(review)\n",
    "    \n",
    "    cv = CountVectorizer(max_features=1500)\n",
    "    X_t = cv.fit_transform(cor1).toarray()\n",
    "\n",
    "    pred = clf.predict([X_t])[-1]\n",
    "\n",
    "    if pred > 0.5 :\n",
    "        print(comment+ ' is a Postive comment!! :) :D\\n')\n",
    "    else:\n",
    "        print(comment+ ' is a Negative Comment!! :(\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " worst food !!! is a Negative Comment!! :(\n",
      "\n",
      " i love it !!! is a Negative Comment!! :(\n",
      "\n",
      " i hate it !!! is a Negative Comment!! :(\n",
      "\n",
      " can be better! is a Postive comment!! :) :D\n",
      "\n",
      "american chopsuey was really good is a Postive comment!! :) :D\n",
      "\n",
      "waiter was rude and didn't get a good service is a Negative Comment!! :(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comment = input('Enter you comment : ')\n",
    "comment = ' worst food !!!'\n",
    "predict_reaction(comment)\n",
    "\n",
    "comment = ' i love it !!!'\n",
    "predict_reaction(comment)\n",
    "\n",
    "comment = ' i hate it !!!'\n",
    "predict_reaction(comment)\n",
    "\n",
    "comment = ' can be better!'\n",
    "predict_reaction(comment)\n",
    "\n",
    "comment = 'american chopsuey was really good'\n",
    "predict_reaction(comment)\n",
    "\n",
    "comment = 'waiter was rude and didn\\'t get a good service'\n",
    "predict_reaction(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less prediction accuracy because of the small dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
